version: 1.0

# -----------------------------
#  RAG PIPELINE SETTINGS
# -----------------------------
rag_pipeline:
  # Paths (relative to REPO_ROOT)
  index_path: "data/vectordb/vector_index.faiss"
  metadata_path: "data/vectordb/metadata.json"
  chunks_path: "data/chunks"
  core_knowledge_path: "data/static/core_knowledge.json"
  
  # Models
  embedding_model: "all-MiniLM-L6-v2"
  reranker_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
  llm_model: "gpt-4o-mini"  # Options: "gpt-3.5-turbo", "gpt-4", "gpt-4-turbo", "gpt-4o", "gpt-4o-mini", or "phi3" for Ollama
  
  # OpenAI Settings (required if using OpenAI models)
  # IMPORTANT: API key should be set via OPENAI_API_KEY environment variable for security
  # Do NOT put your API key in this config file!
  openai:
    # api_key: NOT SET - Use OPENAI_API_KEY environment variable instead
    base_url: null  # Optional: Custom base URL for OpenAI-compatible APIs (e.g., Azure OpenAI)
    temperature: 0.7  # Sampling temperature (0.0 to 2.0)
    max_tokens: null  # Maximum tokens to generate (null = model default)
  
  # Retrieval settings
  top_k_retrieve: 5    # Number of chunks to retrieve initially
  
  # Metadata filtering
  filters:
    enable: false       # Enable/disable automatic category-based filtering
    # When enabled, categories are automatically detected from query keywords
  
  # Reranking settings
  reranker:
    enable: false
    top_k: 5            # Number of top chunks to use after reranking
  
  # Chunking settings (used during ingestion, but kept here for reference)
  chunk_size: 1000       # Default chunk size in characters
  chunk_overlap: 100     # Default chunk overlap in characters
  
  # Verbose output
  verbose: false
